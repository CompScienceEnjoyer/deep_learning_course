{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Создание собственной библиотеки автоматического дифференцирования"
      ],
      "metadata": {
        "id": "JZR7D8renbeZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NNh4KLSSnGoI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None # function\n",
        "        self._prev = set(_children) # set of Value objects\n",
        "        self._op = _op # the op that produced this node, string ('+', '-', ....)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(data = self.data+other.data,\n",
        "                    _children = (self, other),\n",
        "                    _op = \"+\")\n",
        "        def _backward():\n",
        "            if out.grad == None:\n",
        "                self.grad = other.grad*out.grad\n",
        "                other.grad = self.grad*out.grad\n",
        "            else:\n",
        "                self.grad+=out.grad\n",
        "                other.grad+=out.grad\n",
        "            return self.grad+other.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(data = self.data*other.data,\n",
        "                    _children = (self, other),\n",
        "                    _op = \"*\")\n",
        "        def _backward():\n",
        "            if out.grad == None:\n",
        "                self.grad = other.data*out.grad\n",
        "                other.grad = self.data*out.grad\n",
        "            else:\n",
        "                self.grad+=out.grad*other.data\n",
        "                other.grad+=out.grad*self.data\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (float, int)), \"only supporting int/floats power now\"\n",
        "        out = Value(data = self.data**other,\n",
        "                    _children = (self, self),\n",
        "                    _op = \"**\")\n",
        "        def _backward():\n",
        "            if out.grad == None:\n",
        "                self.grad = (self.data ** (other-1))*(other) * out.grad\n",
        "            else:\n",
        "              self.grad+= (self.data ** (other-1))*(other) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __relu__(self):\n",
        "        out = Value(max(self.data, 0),\n",
        "                    _children = (self, self),\n",
        "                    _op = \"&\")\n",
        "        def _backward():\n",
        "            if out.grad == None:\n",
        "                self.grad = min(self.data, 0)\n",
        "            else:\n",
        "                self.grad+=min(self.data, 0) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "5fcF3hGan5kr"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def my_tests():\n",
        "    x = Value(-4.0)\n",
        "    temp_z1 = 2. + x\n",
        "    temp_z2 = x + 2.\n",
        "    temp_z3 = -2. + x\n",
        "    temp_z4 = 2. * x\n",
        "    temp_z5 = x* 2.\n",
        "    z = 2 * x + 2 + x\n",
        "    z.backward()\n",
        "    temp_xmg, temp_zmg = x, z\n",
        "\n",
        "    x = torch.Tensor([-4.0]).double()\n",
        "    x.requires_grad = True\n",
        "    z = 2 * x + 2 + x\n",
        "    z.backward()\n",
        "    temp_xpt, temp_zpt = x, z\n",
        "\n",
        "    # forward pass went well\n",
        "    assert temp_zmg.data == temp_zpt.data.item()\n",
        "    # backward pass went well\n",
        "    print(temp_xmg, temp_xpt, temp_xpt.grad)\n",
        "    assert temp_xmg.grad == temp_xpt.grad.item()\n",
        "\n",
        "    y = Value(-4.0)\n",
        "    temp_h1 = y**4 + 2\n",
        "    temp_h2 = temp_h1\n",
        "    temp_h2.backward()\n",
        "    temp_ypowm, temp_zpowm = y, temp_h2\n",
        "\n",
        "    y = torch.Tensor([-4.0]).double()\n",
        "    y.requires_grad = True\n",
        "    h2 = y**4 + 2\n",
        "    h2.backward()\n",
        "    temp_ypowpt, temp_zpowpt = y, h2\n",
        "    print(temp_zpowm.data)\n",
        "    print(temp_zpowpt.data.item())\n",
        "    # forward pass went well\n",
        "    assert temp_zpowm.data == temp_zpowpt.data.item()\n",
        "    # backward pass went well\n",
        "    print(temp_ypowm)\n",
        "    print(temp_ypowpt, temp_ypowpt.grad)\n",
        "    assert temp_ypowm.grad == temp_ypowpt.grad.item()"
      ],
      "metadata": {
        "id": "1yw-VaKLxpaD"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nuB3efCzwO9",
        "outputId": "3f889095-f994-44c7-ffa0-24640ce4b919"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=-4.0, grad=3) tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([3.], dtype=torch.float64)\n",
            "258.0\n",
            "258.0\n",
            "Value(data=-4.0, grad=-256.0)\n",
            "tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([-256.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sanity_check():\n",
        "\n",
        "    x = Value(-4.0)\n",
        "    z_5 = 2.+x\n",
        "    z_2 = x+2.\n",
        "    z_3 = x*2.\n",
        "    z_4 = 2.*x\n",
        "    z = 2 * x + 2 + x\n",
        "\n",
        "    q = z + z * x                       # Не понял как правильно задать параметр _op для .relu()\n",
        "    h = (z * z)\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xmg, ymg = x, y\n",
        "\n",
        "    x = torch.Tensor([-4.0]).double()\n",
        "    x.requires_grad = True\n",
        "    z = 2 * x + 2 + x\n",
        "    q = z + z * x\n",
        "    h = (z * z)\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xpt, ypt = x, y\n",
        "\n",
        "\n",
        "    # forward pass went well\n",
        "    assert ymg.data == ypt.data.item()\n",
        "    # backward pass went well\n",
        "    print(xmg, xpt, xpt.grad)\n",
        "    assert xmg.grad == xpt.grad.item()\n",
        "\n",
        "\n",
        "def test_more_ops():\n",
        "\n",
        "    a = Value(-4.0)\n",
        "    b = Value(2.0)\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c += c + 1\n",
        "    c += 1 + c + (-a)\n",
        "    d += d * 2 + (b + a)\n",
        "    d += 3 * d + (b - a)\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g += 10.0 / f\n",
        "    g.backward()\n",
        "    amg, bmg, gmg = a, b, g\n",
        "\n",
        "    a = torch.Tensor([-4.0]).double()\n",
        "    b = torch.Tensor([2.0]).double()\n",
        "    a.requires_grad = True\n",
        "    b.requires_grad = True\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c = c + c + 1\n",
        "    c = c + 1 + c + (-a)\n",
        "    d = d + d * 2 + (b + a)\n",
        "    d = d + 3 * d + (b - a)\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g = g + 10.0 / f\n",
        "    g.backward()\n",
        "    apt, bpt, gpt = a, b, g\n",
        "\n",
        "    tol = 1e-6\n",
        "    # forward pass went well\n",
        "    assert abs(gmg.data - gpt.data.item()) < tol\n",
        "    # backward pass went well\n",
        "    assert abs(amg.grad - apt.grad.item()) < tol\n",
        "    assert abs(bmg.grad - bpt.grad.item()) < tol"
      ],
      "metadata": {
        "id": "d91g1om80bVG"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcD5YzrWMTTj",
        "outputId": "4a5253db-56f4-4806-f19b-b4ffc0cb1be1"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=-4.0, grad=27.0) tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([27.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_more_ops()"
      ],
      "metadata": {
        "id": "fAhXYuJRMWhQ"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKjRxIduPEjM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}